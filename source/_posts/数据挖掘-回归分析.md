---
title: 数据挖掘-回归分析
date: 2017-03-07 20:39:09
tags: [DataMining,转载]
cover: "https://raw.githubusercontent.com/ChaoSBYNN/image-hosting/main/program/datamining.jpeg"
---

> 原文
> https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
> 译文
> http://blog.csdn.net/lynnucas/article/details/47948639

------

> 回归分析 : 是一种预测性的建模技术,使用曲线拟合数据点，最终获取到数据点的距离差异最小的曲线

	回归主要三个度量：自变量的个数，因变量的类型以及回归线的形状

# 线性回归 （Linear Regression）

> 因变量是连续的，自变量可以是连续的也可以是离散的，回归线的性质是线性的。

线性回归使用最佳的拟合直线（也就是回归线）在因变量（Y）和一个或多个自变量（X）之间建立一种关系。

用一个方程式来表示它，即Y=a+b*X + e，其中a表示截距，b表示直线的斜率，e是误差项。这个方程可以根据给定的预测变量（s）来预测目标变量的值。

一元线性回归和多元线性回归的区别在于，多元线性回归有（>1）个自变量，而一元线性回归通常只有1个自变量。

* 自变量与因变量之间必须有线性关系
* 多元回归存在多重共线性，自相关性和异方差性。
* 线性回归对异常值非常敏感。它会严重影响回归线，最终影响预测值。
* 多重共线性会增加系数估计值的方差，使得在模型轻微变化下，估计非常敏感。结果就是系数估计值不稳定
* 在多个自变量的情况下，我们可以使用向前选择法，向后剔除法和逐步筛选法来选择最重要的自变量。

# 逻辑回归 （Logistic Regression）

> 逻辑回归是用来计算“事件=Success”和“事件=Failure”的概率。当因变量的类型属于二元（1 / 0，真/假，是/否）变量时，我们就应该使用逻辑回归。

因为在这里我们使用的是的二项分布（因变量），我们需要选择一个对于这个分布最佳的连结函数。它就是Logit函数。在上述方程中，通过观测样本的极大似然估计值来选择参数，而不是最小化平方和误差（如在普通回归使用的）。

* 它广泛的用于分类问题。
* 逻辑回归不要求自变量和因变量是线性关系。它可以处理各种类型的关系，因为它对预测的相对风险指数OR使用了一个非线性的log转换。
* 为了避免过拟合和欠拟合，我们应该包括所有重要的变量。有一个很好的方法来确保这种情况，就是使用逐步筛选方法来估计逻辑回归。
* 它需要大的样本量，因为在样本数量较少的情况下，极大似然估计的效果比普通的最小二乘法差。
* 自变量不应该相互关联的，即不具有多重共线性。然而，在分析和建模中，我们可以选择包含分类变量相互作用的影响。
* 如果因变量的值是定序变量，则称它为序逻辑回归。
* 如果因变量是多类的话，则称它为多元逻辑回归。

# 多项式回归 （Polynomial Regression）

> 对于一个回归方程，如果自变量的指数大于1，那么它就是多项式回归方程。

在这种回归模型中，最佳拟合线不是直线。而是一个用于拟合数据点的曲线。

* 虽然会有一个诱导可以拟合一个高次多项式并得到较低的错误，但这可能会导致过拟合。你需要经常画出关系图来查看拟合情况，并且专注于保证拟合合理，既没有过拟合又没有欠拟合。
* 明显地向两端寻找曲线点，看看这些形状和趋势是否有意义。更高次的多项式最后可能产生怪异的推断结果。

# 逐步回归 （Stepwise Regression）

> 在处理多个自变量时，我们可以使用这种形式的回归。

这一壮举是通过观察统计的值，如R-square，t-stats和AIC指标，来识别重要的变量。逐步回归通过同时添加/删除基于指定标准的协变量来拟合模型。下面列出了一些最常用的逐步回归方法：

* 标准逐步回归法做两件事情。即增加和删除每个步骤所需的预测。
* 向前选择法从模型中最显著的预测开始，然后为每一步添加变量。
* 向后剔除法与模型的所有预测同时开始，然后在每一步消除最小显着性的变量。

这种建模技术的目的是使用最少的预测变量数来最大化预测能力。这也是处理高维数据集的方法之一。

# 岭回归 （Ridge Regression）

> 岭回归分析是一种用于存在多重共线性（自变量高度相关）数据的技术。

在多重共线性情况下，尽管最小二乘法（OLS）对每个变量很公平，但它们的差异很大，使得观测值偏移并远离真实值。岭回归通过给回归估计上增加一个偏差度，来降低标准误差。

在一个线性方程中，预测误差可以分解为2个子分量。一个是偏差，一个是方差。预测错误可能会由这两个分量或者这两个中的任何一个造成。

* 除常数项以外，这种回归的假设与最小二乘回归类似；
* 它收缩了相关系数的值，但没有达到零，这表明它没有特征选择功能
* 这是一个正则化方法，并且使用的是L2正则化。

# 套索回归 （Lasso Regression）

> 它类似于岭回归，Lasso （Least Absolute Shrinkage and Selection Operator）也会惩罚回归系数的绝对值大小。

此外，它能够减少变化程度并提高线性回归模型的精度。

Lasso 回归与Ridge回归有一点不同，它使用的惩罚函数是绝对值，而不是平方。这导致惩罚（或等于约束估计的绝对值之和）值使一些参数估计结果等于零。使用惩罚值越大，进一步估计会使得缩小值趋近于零。这将导致我们要从给定的n个变量中选择变量。

* 除常数项以外，这种回归的假设与最小二乘回归类似；
* 它收缩系数接近零（等于零），这确实有助于特征选择；
* 这是一个正则化方法，使用的是L1正则化；

如果预测的一组变量是高度相关的，Lasso 会选出其中一个变量并且将其它的收缩为零。

# 弹性网络回归（ElasticNet Regression）

> ElasticNet是Lasso和Ridge回归技术的混合体。

它使用L1来训练并且L2优先作为正则化矩阵。当有多个相关的特征时，ElasticNet是很有用的。Lasso 会随机挑选他们其中的一个，而ElasticNet则会选择两个。

Lasso和Ridge之间的实际的优点是，它允许ElasticNet继承循环状态下Ridge的一些稳定性。

* 在高度相关变量的情况下，它会产生群体效应；
* 选择变量的数目没有限制；
* 它可以承受双重收缩。